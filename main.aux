\relax 
\abx@aux@refcontext{nty/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Neural network}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Foward and Backward pass}{2}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Procedure to find the optimal solution in Multi-layer Neural Network}}{2}{figure.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Forward Pass}{3}{subsection.1.1.1}\protected@file@percent }
\newlabel{eq:neuron_output}{{1.2}{3}{Forward Pass}{equation.1.1.2}{}}
\newlabel{eq:neuron_output_matrix_form}{{1.3}{3}{Forward Pass}{equation.1.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Illustration input and output of a neuron}}{4}{figure.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Backward pass (Backpropagation)}{4}{subsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Calculating the error of output layer}}{4}{figure.1.3}\protected@file@percent }
\newlabel{eq:erroroutput}{{1.5}{4}{Backward pass (Backpropagation)}{equation.1.1.5}{}}
\newlabel{eq:errorhidden}{{1.7}{5}{Backward pass (Backpropagation)}{equation.1.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Calculating the error of hidden layers}}{5}{figure.1.4}\protected@file@percent }
\newlabel{fig:errorlayer}{{1.4}{5}{Calculating the error of hidden layers}{figure.1.4}{}}
\newlabel{eq:errorbias}{{1.8}{5}{Backward pass (Backpropagation)}{equation.1.1.8}{}}
\newlabel{eq:errorweight}{{1.9}{6}{Backward pass (Backpropagation)}{equation.1.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Optimizer}{6}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Gradient Descent and its variants}{6}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent (SGD)}{6}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Gradient Descent}{6}{equation.1.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mini-Batch Gradient Descent}{7}{equation.1.2.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent with Momentum}{7}{equation.1.2.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Illustration for big \textit  {learning rate} and small \textit  {learning rate} situation. In the former case, the red points are the attemps of neural network model to approach optimized point (green point) but large LR occurs the next update value keep jumpng around the global bottom point. In the later case, we can see multiple attemps to pass the local maxima but because of small LR, the updated effort is very slow.}}{8}{figure.1.5}\protected@file@percent }
\newlabel{fig:issueLR}{{1.5}{8}{Illustration for big \textit {learning rate} and small \textit {learning rate} situation. In the former case, the red points are the attemps of neural network model to approach optimized point (green point) but large LR occurs the next update value keep jumpng around the global bottom point. In the later case, we can see multiple attemps to pass the local maxima but because of small LR, the updated effort is very slow}{figure.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Nesterov Accelerated Gradient}{8}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Adaptive Gradient Algorithm (Adagrad)}{9}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Root Mean Square Propagation (RMSProp)}{9}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Adaptive Moment Estimation (Adam) and its variants}{9}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adaptive Moment Estimation (Adam)}{9}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adam with Weight Decay (AdamW)}{9}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{AdamMax}{9}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Nesterov-accelerated Adam (Nadam)}{9}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Activation function}{9}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Sigmoid function}{9}{subsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Sigmoid function}}{9}{figure.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Softmax function}{9}{subsection.1.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Softmax function}}{10}{figure.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Hyperbolic Tangent (Tanh) function}{10}{subsection.1.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces TanH function}}{10}{figure.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Rectifier Unit (ReLU) function}{10}{subsection.1.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces ReLU function}}{11}{figure.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Leaky Rectifier Unit (LeakyReLU) function}{11}{subsection.1.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Leaky ReLU function}}{11}{figure.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Exponential Linear Unit (ELU) function}{11}{subsection.1.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces ELU function}}{12}{figure.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Gaussian Error Linear Unit (GELU) function}{12}{subsection.1.3.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces GELU function}}{12}{figure.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.8}Swish function}{12}{subsection.1.3.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces GELU function}}{13}{figure.1.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.9}Maxout function}{13}{subsection.1.3.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Maxout function}}{13}{figure.1.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Loss function}{13}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Mean Squared Error (MSE) loss}{13}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Mean Absolute Error (MSA) loss}{13}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Huber loss}{14}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Cross-Entropy loss and its variants}{14}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross-Entropy loss}{14}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Categorical Cross-Entropy loss}{14}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sparse Categorical Cross-Entropy loss}{14}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Dice loss}{14}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Focal loss}{14}{subsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Triplet loss}{14}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}Tversky loss}{14}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Proved of equation in backpropagation}{14}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Convolutional Neural Network (CNN)}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convolutional Neural Network}{15}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Vectorized convolution by Im2col and col2im}{15}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Convolution operation between 7x7 input and 3x3 kernel filter with $stride=1$. If we use a loop then each iteration we slide over a 3x3 subgrid of input, we have to do9 multiplications and 8 addition. The number operations we have to do is $9x8x25=1800$ operations for 1 convolution. Remind that with this looping implementation, we can only do $9x8=72$ operations sequentially.}}{16}{figure.2.1}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{D41D8CD98F00B204E9800998ECF8427E}
\gdef \@abspage@last{17}
