\relax 
\abx@aux@refcontext{nty/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Neural network}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Foward and Backward pass}{2}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Procedure to find the optimal solution in Multi-layer Neural Network}}{2}{figure.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Forward Pass}{3}{subsection.1.1.1}\protected@file@percent }
\newlabel{eq:neuron_output}{{1.2}{3}{Forward Pass}{equation.1.1.2}{}}
\newlabel{eq:neuron_output_matrix_form}{{1.3}{3}{Forward Pass}{equation.1.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Illustration input and output of a neuron}}{4}{figure.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Backward pass (Backpropagation)}{4}{subsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Calculating the error of output layer}}{4}{figure.1.3}\protected@file@percent }
\newlabel{eq:erroroutput}{{1.5}{4}{Backward pass (Backpropagation)}{equation.1.1.5}{}}
\newlabel{eq:errorhidden}{{1.7}{5}{Backward pass (Backpropagation)}{equation.1.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Calculating the error of hidden layers}}{5}{figure.1.4}\protected@file@percent }
\newlabel{fig:errorlayer}{{1.4}{5}{Calculating the error of hidden layers}{figure.1.4}{}}
\newlabel{eq:errorbias}{{1.8}{5}{Backward pass (Backpropagation)}{equation.1.1.8}{}}
\newlabel{eq:errorweight}{{1.9}{6}{Backward pass (Backpropagation)}{equation.1.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Optimizer}{6}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Gradient Descent and its variants}{6}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent (SGD)}{6}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Gradient Descent}{6}{equation.1.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mini-Batch Gradient Descent}{7}{equation.1.2.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent with Momentum}{7}{equation.1.2.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Nesterov Accelerated Gradient}{7}{subsection.1.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Illustration for big \textit  {learning rate} and small \textit  {learning rate} situation. In the former case, the red points are the attemps of neural network model to approach optimized point (green point) but large LR occurs the next update value keep jumpng around the global bottom point. In the later case, we can see multiple attemps to pass the local maxima but because of small LR, the updated effort is very slow.}}{8}{figure.1.5}\protected@file@percent }
\newlabel{fig:issueLR}{{1.5}{8}{Illustration for big \textit {learning rate} and small \textit {learning rate} situation. In the former case, the red points are the attemps of neural network model to approach optimized point (green point) but large LR occurs the next update value keep jumpng around the global bottom point. In the later case, we can see multiple attemps to pass the local maxima but because of small LR, the updated effort is very slow}{figure.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Adaptive Gradient Algorithm (Adagrad)}{8}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Root Mean Square Propagation (RMSProp)}{8}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Adaptive Moment Estimation (Adam) and its variants}{8}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adaptive Moment Estimation (Adam)}{8}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adam with Weight Decay (AdamW)}{8}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{AdamMax}{8}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Nesterov-accelerated Adam (Nadam)}{8}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Activation function}{8}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Sigmoid function}{8}{subsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Sigmoid function}}{9}{figure.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Softmax function}{9}{subsection.1.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Softmax function}}{9}{figure.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Hyperbolic Tangent (Tanh) function}{9}{subsection.1.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces TanH function}}{10}{figure.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Rectifier Unit (ReLU) function}{10}{subsection.1.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces ReLU function}}{10}{figure.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Leaky Rectifier Unit (LeakyReLU) function}{10}{subsection.1.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Leaky ReLU function}}{11}{figure.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Exponential Linear Unit (ELU) function}{11}{subsection.1.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces ELU function}}{11}{figure.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Gaussian Error Linear Unit (GELU) function}{11}{subsection.1.3.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces GELU function}}{12}{figure.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.8}Swish function}{12}{subsection.1.3.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces GELU function}}{12}{figure.1.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.9}Maxout function}{12}{subsection.1.3.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Maxout function}}{13}{figure.1.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Loss function}{13}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Mean Squared Error (MSE) loss}{13}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Mean Absolute Error (MSA) loss}{13}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Huber loss}{13}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Cross-Entropy loss and its variants}{13}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross-Entropy loss}{13}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Categorical Cross-Entropy loss}{13}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Sparse Categorical Cross-Entropy loss}{13}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Dice loss}{14}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Focal loss}{14}{subsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Triplet loss}{14}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}Tversky loss}{14}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Proved of equation in backpropagation}{14}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Convolutional Neural Network (CNN)}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Convolutional Neural Network}{15}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Vectorized convolution by Im2col and col2im}{15}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Convolution operation between $7\times 7$ input and $3\times 3$ kernel filter with $stride=1$. If we use a loop then each iteration we slide over a $3\times 3$ subgrid of input, we have to do9 multiplications and 8 addition. The number operations we have to do is $9\times 8\times 25=1800$ operations for 1 convolution. Remind that with this looping implementation, we can only do $9\times 8=72$ operations sequentially.}}{16}{figure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Step1: Image to Column (Im2Col) operation is to sort all values in convolutional kernel in a column}}{17}{figure.2.2}\protected@file@percent }
\newlabel{fig:im2col}{{2.2}{17}{Step1: Image to Column (Im2Col) operation is to sort all values in convolutional kernel in a column}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The output Im2col in 3-channels image}}{17}{figure.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Reshape the kernel to row vector form. Given there are N kernels, then the reshaped kernels set will have the concatenated row vectors}}{18}{figure.2.4}\protected@file@percent }
\newlabel{fig:reshaped_kernels}{{2.4}{18}{Reshape the kernel to row vector form. Given there are N kernels, then the reshaped kernels set will have the concatenated row vectors}{figure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Reshape kernel in 3-channels }}{18}{figure.2.5}\protected@file@percent }
\newlabel{fig:reshaped_kernels_3_channels}{{2.5}{18}{Reshape kernel in 3-channels}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Matrix multiplication operation between reshaped kernels and Im2Col output. The output of this operation will be reshaped to the expected form as the same as after convolution.}}{18}{figure.2.6}\protected@file@percent }
\newlabel{fig:matrix_multiplication}{{2.6}{18}{Matrix multiplication operation between reshaped kernels and Im2Col output. The output of this operation will be reshaped to the expected form as the same as after convolution}{figure.2.6}{}}
\abx@aux@read@bbl@mdfivesum{EE04F40F768DAEED26442158E95ABA61}
\gdef \@abspage@last{19}
